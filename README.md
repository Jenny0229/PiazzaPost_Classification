# PiazzaPost_Classification

## Intro
In this project, I redid the final project of EECS280(in Umich) that wrote a machine learning program to automatically give tags to posts on Piazza(a class forum where students can post questions and instructors and other students can answer). Although that project was called machine learning, we were basically following the step-by-step tutorial given by our instructors and the main thing we were tested on were data structures(maps, binary search trees etc) with C++ rather than machine learning. So after learning a lot about machine learning this summer, I decided to do a real machine-learning project using Python to get my hands dirty. Reference materials for this project include “Hands-on Machine Learning with Scikit-learn, Keras & Tensorflow” by Aurelien Geron, “Natural Language Processing with Python” by Steven Bird et al and other online tutorials.

## Overview
The data was divided into two parts: posts classified with two tags(instructor and student) based on who sent the post and posts classified into multiple tags based on the content. I divided the binary classifier and multiclass classifier into two notebooks, named “BinaryClassification_StuIns" and "Multiclass_ExamProejct" respectively. The basic pipeline for the models is the same: loading data -> exploratory data analysis -> preprocessing(removing stopwords, stemmer, tokenization) -> model training -> fine-tuning -> analyze performance. 

## Binary Classification
In EDA phase, I made a couple of histograms to see the distribution of the dataset, calculated the length for each post and made a couple of graphs about them, and generated two word clouds. I found the dataset highly imbalanced, with student: instructor = 13940:413. As I can't resample the data, the best solution I can come up with is to do stratified sampling when doing validation and do downsampling on the majority class or upsampling on the minority class later. For preprocessing, I removed the stop words and used porter stemmer to stem the posts. After test_train_split I transformed the posts into tfidf vectors for model training. I trained a multinomial Bayes model but the performance was rather poor. The recall for "instructor" was extremely low. I guess that was because the dataset was so imbalanced. I did the two strategies mentioned above but they did not do much good. 

## Multiclass Classification
I did basically the same thing at EDA and preprocessing phase as the binary classifier above. This time the dataset is rather balanced and I trained three models with tfidf transformer. The three models are multinomial Bayes model, logistic regression model, and linear SVM. I also fine-tuned the model by using cross-validation and used grid search for the best hyperparameter. The performance of logistic regression and SVM were similar, with accuracy score at around 0.76(SVM was slightly higher) while the Bayes model performed worse than those two. Next, I also tried the neural network to embed the words(word2vec) and trained a logistic regression model based on that. Surprisingly that model didn't perform as well as the two before(I thought it would definitely do better since the neural network does better on context analysis), perhaps it's because my dataset is too small to effectively use the word2vec embedding.
